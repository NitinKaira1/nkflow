# nkflow

nkflow is a Python library that enables users to generate code using OpenAI GPT models, save the generated code to files, and optionally execute Python scripts or open HTML files in a browser. It supports session-based memory for multi-step tasks, making it ideal for iterative code generation workflows.

## Features

- **AI-powered code generation** using OpenAI GPT models
- **Save generated code** to files automatically
- **Run Python scripts** or **open HTML files** in your browser
- **Session-based memory** for multi-step tasks
- Simple API for integration into your projects

## Installation

```bash
pip install langchain langchain-openai
pip install nkflow
# Add yourlib installation if published, e.g.:
# pip install nkflow
```

## Usage

```python
from langchain_openai import ChatOpenAI
from nkflow import run

openai_model = ChatOpenAI(model="gpt-4o-mini", api_key="XYZ")
run("Make me a todo-list website", "todo.html", False, llm=openai_model)
```

## Parameters

| Parameter    | Type     | Description                                                                 | Default        |
|--------------|----------|-----------------------------------------------------------------------------|----------------|
| `task`       | `str`    | The instruction or task for code generation                                 | —              |
| `filename`   | `str`    | The file path to save the generated code                                    | —              |
| `auto_run`   | `bool`   | If `True`, runs Python scripts or opens HTML files after generation         | `False`        |
| `llm`        | object   | The language model instance (e.g., `ChatOpenAI`)                            | —              |
| `session_id` | `str`    | Session identifier for multi-step tasks                                     | `"single_run"` |

## How It Works

Pyflow uses a combination of prompt engineering and session-based memory to interact with OpenAI GPT models for code generation. Here’s a breakdown of the workflow:

1. **Session Memory**: Each session is tracked using a unique `session_id`. This enables multi-step tasks and preserves conversation history using `ChatMessageHistory`.
2. **Prompt Template**: The system prompt instructs the LLM to return only code, wrapped in triple backticks, and to split long code into labeled parts.
3. **Code Extraction**: The response from the LLM is parsed using regex to extract code blocks and their language type.
4. **Saving Code**: Extracted code is saved to the specified filename.
5. **Optional Execution**: If `auto_run` is `True`, Python scripts are executed via `subprocess`, and HTML files are opened in the default browser.
6. **LLM Agnostic**: The main function `run` works with any compatible language model, such as `ChatOpenAI`.

### Internal Workflow Example

```python
def run(task: str, filename: str, auto_run: bool, llm, session_id="single_run"):
    # Build chain with user-supplied LLM and session memory
    chain = RunnableWithMessageHistory(
        _prompt | llm,
        _get_memory,
        input_messages_key="input",
        history_messages_key="history",
    )
    # Query LLM
    response = chain.invoke(
        {"input": f"Task: {task}"},
        config={"configurable": {"session_id": session_id}}
    )
    # Extract and save code
    code_blocks = _extract_code(response.content)
    full_code = "\n\n".join(code for _, code in code_blocks)
    with open(filename, "w", encoding="utf-8") as f:
        f.write(full_code)
    # Optionally run or open file
    if auto_run:
        if filename.endswith(".py"):
            subprocess.run(["python", filename], check=False)
        elif filename.endswith(".html"):
            webbrowser.open_new_tab(filename)
```

## Example Output

After running:

```python
run("Make me a todo-list website", "todo.html", False, llm=openai_model)
```

You will find a `todo.html` file containing a complete HTML/CSS/JS todo-list website generated by GPT.

## Contributing

Contributions are welcome! Please open issues or submit pull requests via GitHub.

## License

This project is licensed under the MIT License.

## Notes

- Requires Python 3.9 or higher.
- Ensure you have a valid OpenAI API key.
- For advanced usage, refer to the documentation of `langchain` and `langchain-openai`.
- The code extraction uses regex to reliably parse code blocks from LLM responses.
- Session-based memory enables multi-step, context-aware code generation.
- The API is designed to be simple and extensible for various code generation tasks.
